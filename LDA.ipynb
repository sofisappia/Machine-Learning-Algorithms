{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.backends:backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as spio\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import matplotlib as mpl\n",
    "from os import path\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from wyrm.types import Data\n",
    "\n",
    "from wyrm import plot\n",
    "#plot.beautify()\n",
    "from wyrm.types import Data\n",
    "from wyrm import processing as proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_A = '../BCI_Comp_III_Wads_2004/data/Subject_A_Train.mat'\n",
    "TRAIN_B = '../BCI_Comp_III_Wads_2004/data/Subject_B_Train.mat'\n",
    "\n",
    "TEST_A = '../BCI_Comp_III_Wads_2004/data/Subject_A_Test.mat'\n",
    "TEST_B = '../BCI_Comp_III_Wads_2004/data/Subject_B_Test.mat'\n",
    "\n",
    "TRUE_LABELS_A = 'WQXPLZCOMRKO97YFZDEZ1DPI9NNVGRQDJCUVRMEUOOOJD2UFYPOO6J7LDGYEGOA5VHNEHBTXOO1TDOILUEE5BFAEEXAW_K4R3MRU'\n",
    "TRUE_LABELS_B = 'MERMIROOMUHJPXJOHUVLEORZP3GLOO7AUFDKEFTWEOOALZOP9ROCGZET1Y19EWX65QUYU7NAK_4YCJDVDNGQXODBEV2B5EFDIDNR'\n",
    "\n",
    "MATRIX = ['abcdef',\n",
    "          'ghijkl',\n",
    "          'mnopqr',\n",
    "          'stuvwx',\n",
    "          'yz1234',\n",
    "          '56789_']\n",
    "\n",
    "MARKER_DEF_TRAIN = {'target': ['target'], 'nontarget': ['nontarget']}\n",
    "MARKER_DEF_TEST = {'flashing': ['flashing']}\n",
    "\n",
    "SEG_IVAL = [0, 700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bci_data(filename):\n",
    "    \"\"\"Load the BCI Competition III Data Set 2.\n",
    "    This method loads the data set and converts it into Wyrm's ``Data``\n",
    "    format. Before you use it, you have to download the data set in\n",
    "    Matlab format and unpack it. The directory with the extracted files\n",
    "    must contain the ``Subject_*.mat``- and the ``eloc64.txt`` files.\n",
    "    .. note::\n",
    "        If you need the true labels of the test sets, you'll have to\n",
    "        download them separately from\n",
    "        http://bbci.de/competition/iii/results/index.html#labels\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The path to the matlab file to load\n",
    "    Returns\n",
    "    -------\n",
    "    cnt : continuous `Data` object\n",
    "    Examples\n",
    "    --------\n",
    "    >>> dat = load_bcicomp3_ds2('/home/foo/data/Subject_A_Train.mat')\n",
    "    \"\"\"\n",
    "    STIMULUS_CODE = {\n",
    "        0 : \"blankMatrix\",\n",
    "        # cols from left to right\n",
    "        1 : \"agmsy5\",\n",
    "        2 : \"bhntz6\",\n",
    "        3 : \"ciou17\",\n",
    "        4 : \"djpv28\",\n",
    "        5 : \"ekqw39\",\n",
    "        6 : \"flrx4_\",\n",
    "        # rows from top to bottom\n",
    "        7 : \"abcdef\",\n",
    "        8 : \"ghijkl\",\n",
    "        9 : \"mnopqr\",\n",
    "        10: \"stuvwx\",\n",
    "        11: \"yz1234\",\n",
    "        12: \"56789_\"\n",
    "        }\n",
    "\n",
    "    # load the matlab data\n",
    "    data_mat = loadmat(filename)\n",
    "    # load the channel names (the same for all datasets\n",
    "    eloc_file = path.sep.join([path.dirname(filename), 'eloc64.txt'])\n",
    "    with open(eloc_file) as fh:\n",
    "        data = fh.read()\n",
    "    channels = []\n",
    "    for line in data.splitlines():\n",
    "        if line:\n",
    "            chan = line.split()[-1]\n",
    "            chan = chan.replace('.', '')\n",
    "            channels.append(chan)\n",
    "    # fix the channel names, some letters have the wrong capitalization\n",
    "    for i, s in enumerate(channels):\n",
    "        s2 = s.upper()\n",
    "        s2 = s2.replace('Z', 'z')\n",
    "        s2 = s2.replace('FP', 'Fp')\n",
    "        channels[i] = s2\n",
    "    # The signal is recorded with 64 channels, bandpass filtered\n",
    "    # 0.1-60Hz and digitized at 240Hz. The format is Character Epoch x\n",
    "    # Samples x Channels\n",
    "    data = data_mat['Signal']\n",
    "    data = data.astype('double')\n",
    "    # For each sample: 1 if a row/colum was flashed, 0 otherwise\n",
    "    flashing = data_mat['Flashing'].reshape(-1)\n",
    "    #flashing = np.flatnonzero((np.diff(a) == 1)) + 1\n",
    "    ##Creates an array where only the initial intensifications of each series appear\n",
    "    tmp = []\n",
    "    for i, _ in enumerate(flashing):\n",
    "        if i == 0:\n",
    "            tmp.append(flashing[i])\n",
    "            continue\n",
    "        if flashing[i] == flashing[i-1] == 1:\n",
    "            tmp.append(0)\n",
    "            continue\n",
    "        tmp.append(flashing[i])\n",
    "    flashing = np.array(tmp)\n",
    "    # For each sample: 0 when no row/colum was intensified,\n",
    "    # 1..6 for intensified columns, 7..12 for intensified rows\n",
    "    stimulus_code = data_mat['StimulusCode'].reshape(-1)\n",
    "    stimulus_code = stimulus_code[flashing == 1]\n",
    "    # 0 if no row/col was intensified or the intensified did not contain\n",
    "    # the target character, 1 otherwise\n",
    "    stimulus_type = data_mat.get('StimulusType', np.array([])).reshape(-1)\n",
    "    # The target characters\n",
    "    target_chars = data_mat.get('TargetChar', np.array([])).reshape(-1)\n",
    "    fs = 240\n",
    "    data = data.reshape(-1, 64)\n",
    "    timeaxis = np.linspace(0, data.shape[0] / fs * 1000, data.shape[0], endpoint=False)\n",
    "    dat = Data(data=data, axes=[timeaxis, channels], names=['time', 'channel'], units=['ms', '#'])\n",
    "    dat.fs = fs\n",
    "    # preparing the markers\n",
    "    target_mask = np.logical_and((flashing == 1), (stimulus_type == 1)) if len(stimulus_type) > 0 else []\n",
    "    nontarget_mask = np.logical_and((flashing == 1), (stimulus_type == 0)) if len(stimulus_type) > 0 else []\n",
    "    flashing = (flashing == 1)\n",
    "    flashing = [[i, 'flashing'] for i in timeaxis[flashing]]\n",
    "    targets = [[i, 'target'] for i in timeaxis[target_mask]]\n",
    "    nontargets = [[i, 'nontarget'] for i in timeaxis[nontarget_mask]]\n",
    "    dat.stimulus_code = stimulus_code[:]\n",
    "    stim = []\n",
    "    for i,_ in enumerate(flashing):\n",
    "        stim.append([flashing[i][0], STIMULUS_CODE[stimulus_code[i]]])\n",
    "    stimulus_code = stim\n",
    "    #stimulus_code = zip([t for t, _ in flashing], [STIMULUS_CODE[i] for i in stimulus_code])\n",
    "    #Raises error \"TypeError: '<' not supported between instances of 'tuple' and 'list'\" when calling sort() \n",
    "    #stimulus_code =[[t for t,_ in flashing], [STIMULUS_CODE[i] for i in stimulus_code]]\n",
    "    #print(type(stimulus_code), type(flashing), type(targets), type(nontargets))\n",
    "    markers = flashing[:]\n",
    "    markers.extend(targets)\n",
    "    markers.extend(nontargets)\n",
    "    markers.extend(stimulus_code)\n",
    "    markers.sort()\n",
    "    dat.markers = markers[:]\n",
    "    return dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_simple(dat, MRK_DEF, *args, **kwargs):\n",
    "    \"\"\"Simple preprocessing that reaches 97% accuracy.\n",
    "    \"\"\"\n",
    "    fs_n = dat.fs / 2\n",
    "    b, a = proc.signal.butter(5, [10 / fs_n], btype='low')\n",
    "    dat = proc.filtfilt(dat, b, a)\n",
    "   \n",
    "    dat = proc.subsample(dat, 20)\n",
    "    epo = proc.segment_dat(dat, MRK_DEF, SEG_IVAL)\n",
    "    fv = proc.create_feature_vectors(epo)\n",
    "    return fv, epo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_train = load_bci_data(TRAIN_A)\n",
    "dat_test = load_bci_data(TEST_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:wyrm.processing:Subsampling led to loss of 6 samples, in an online setting consider using a BlockBuffer with a buffer size of a multiple of 12 samples.\n"
     ]
    }
   ],
   "source": [
    "fv_train, epo_train = preprocessing_simple(dat_train, MARKER_DEF_TRAIN, SEG_IVAL)\n",
    "fv_test, _ = preprocessing_simple(dat_test, MARKER_DEF_TEST, SEG_IVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA algorithm feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fv_train.data #data\n",
    "y = fv_train.axes[0] #labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means of each class\n",
    "mu1 = np.matrix(np.mean(x[y == 0], axis = 0)) #non target\n",
    "mu2 = np.matrix(np.mean(x[y == 1], axis = 0)) #target\n",
    "\n",
    "n1 = x[y == 0].shape[0]\n",
    "n2 = x[y == 1].shape[0]\n",
    "N = n1 + n2\n",
    "\n",
    "# Total mean\n",
    "mu_total = (n1 * mu1 + n2 * mu2)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Between class matrix\n",
    "Sb1 = np.transpose(mu1 - mu_total) * (mu1 - mu_total) \n",
    "Sb2 = np.transpose(mu2 - mu_total) * (mu2 - mu_total) \n",
    "\n",
    "Sb = n1 * Sb1 + n2 * Sb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within class matrix\n",
    "d1 = np.matrix(x[y == 0]) - mu1\n",
    "d2 = np.matrix(x[y == 1]) - mu2\n",
    "Sw1 = np.transpose(d1) * (d1)\n",
    "Sw2 = np.transpose(d2) * (d2)\n",
    "\n",
    "Sw = Sw1 + Sw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation matrix\n",
    "W = np.linalg.inv(Sw) * Sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvalues and eigenvectors\n",
    "lambda_, V = np.linalg.eig(W)\n",
    "idxs = lambda_.argsort()\n",
    "V = V[:,idxs[-5:]] # Most robust eigenvector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    'Linear Discriminant Classifier'\n",
    "    def __init__(self):\n",
    "        w = np.array([])\n",
    "        b = np.array([])\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Fit LinearDiscriminantAnalysis model according\n",
    "        to the given training data.'\n",
    "        \n",
    "        Arguments: \n",
    "        ----------\n",
    "        x: array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y: array, shape (n_samples)\n",
    "            Training labels\n",
    "        \"\"\"\n",
    "    \n",
    "        # Means for each class\n",
    "        mu1 = x[y == 0].mean(axis = 0)\n",
    "        mu2 = x[y == 1].mean(axis = 0)\n",
    "        mu = mu1 + mu2\n",
    "\n",
    "        # Covariance matrices for each class\n",
    "        cov1 = np.cov(x[y == 0], rowvar = False)\n",
    "        cov2 = np.cov(x[y == 1], rowvar = False)\n",
    "        covm = cov1 + cov2\n",
    "\n",
    "        # Weight vector\n",
    "        self.w = np.dot(np.linalg.pinv(covm), (mu2 - mu1))\n",
    "\n",
    "        # Bias term\n",
    "        self.b = -0.5 * np.dot(self.w.transpose(), mu)\n",
    "        \n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"Estimates the probability\n",
    "        Arguments: \n",
    "        ----------\n",
    "        x: array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        Returns:\n",
    "        --------\n",
    "        y: array, shape (n_samples)\n",
    "            Estimated probabilities\n",
    "        \"\"\"\n",
    "        y = np.dot(x,self.w) + self.b\n",
    "        return y\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_test = fv_test.data\n",
    "clf = LDA()\n",
    "clf.fit(x, y)\n",
    "pred = clf.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_character(pred, n_characters, labels):\n",
    "    pred_target = pred\n",
    "    #unscramble_idx = fv_test.stimulus_code.reshape(100, 15, 12).argsort()\n",
    "    unscramble_idx = fv_test.stimulus_code.reshape(n_characters, -1, 12).argsort()\n",
    "    static_idx = np.indices(unscramble_idx.shape)\n",
    "    #lda_out_prob = pred.reshape(100, 15, 12)\n",
    "    lda_out_prob = pred.reshape(n_characters, -1, 12)\n",
    "    lda_out_prob = lda_out_prob[static_idx[0], static_idx[1], unscramble_idx]\n",
    "\n",
    "    # destil the result of the 15 runs\n",
    "    lda_out_prob = lda_out_prob.sum(axis=1)\n",
    "    lda_out_prob = lda_out_prob.argsort()\n",
    "\n",
    "\n",
    "    cols = lda_out_prob[lda_out_prob <= 5].reshape(n_characters, -1)\n",
    "    rows = lda_out_prob[lda_out_prob > 5].reshape(n_characters, -1)\n",
    "    text = ''\n",
    "    for i in range(n_characters):\n",
    "        row = rows[i][-1]-6\n",
    "        col = cols[i][-1]\n",
    "        letter = MATRIX[row][col]\n",
    "        text += letter\n",
    "    print()\n",
    "    print('Constructed labels: %s' % text.upper())\n",
    "    print('True labels       : %s' % labels)\n",
    "    a = np.array(list(text.upper()))\n",
    "    b = np.array(list(TRUE_LABELS_A))\n",
    "    accuracy = np.count_nonzero(a == b) / len(a)\n",
    "    print('Accuracy: %.1f%%' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constructed labels: WQXPLZCOMRKOW7YFZDEZ1DPI9NNVGRPDJCUVRMEUOOOJD2UFYPOO6J7LDGYEGOA5VTNEHBWXOO1TDOILUEE5BFAFEXAW_K3R3MRU\n",
      "True labels       : WQXPLZCOMRKO97YFZDEZ1DPI9NNVGRQDJCUVRMEUOOOJD2UFYPOO6J7LDGYEGOA5VHNEHBTXOO1TDOILUEE5BFAEEXAW_K4R3MRU\n",
      "Accuracy: 94.0%\n"
     ]
    }
   ],
   "source": [
    "predict_character(pred, 100, TRUE_LABELS_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Scikitlearn LDA and Mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3.398927415973617 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "cf = LinearDiscriminantAnalysis()\n",
    "t_start = time.clock()\n",
    "cf.fit(x,y)\n",
    "pred1 = cf.predict_proba(x_test)\n",
    "t_end = time.clock()\n",
    "\n",
    "print('Time: {} s'.format(t_end - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.089816254420981 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "cf = LDA()\n",
    "t_start = time.clock()\n",
    "cf.fit(x,y)\n",
    "pred2 = cf.predict_proba(x_test)\n",
    "t_end = time.clock()\n",
    "\n",
    "print('Time: {} s'.format(t_end - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sklearn LDA\n",
      "\n",
      "Constructed labels: WQXPLZCOMRKO97YFZDEZ1DPI9NNVGRQDJCUVRMEUOUOJD2UFYPOO6J7LDAYEGOA5VHNE9BWXOO1TDOILUEE5BFAREXAWRK4R3MRU\n",
      "True labels       : WQXPLZCOMRKO97YFZDEZ1DPI9NNVGRQDJCUVRMEUOOOJD2UFYPOO6J7LDGYEGOA5VHNEHBTXOO1TDOILUEE5BFAEEXAW_K4R3MRU\n",
      "Accuracy: 94.0%\n"
     ]
    }
   ],
   "source": [
    "print('\\nSklearn LDA')\n",
    "predict_character(pred1[:,1], 100, TRUE_LABELS_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My LDA\n",
      "\n",
      "Constructed labels: WQXPLZCOMRKOW7YFZDEZ1DPI9NNVGRPDJCUVRMEUOOOJD2UFYPOO6J7LDGYEGOA5VTNEHBWXOO1TDOILUEE5BFAFEXAW_K3R3MRU\n",
      "True labels       : WQXPLZCOMRKO97YFZDEZ1DPI9NNVGRQDJCUVRMEUOOOJD2UFYPOO6J7LDGYEGOA5VHNEHBTXOO1TDOILUEE5BFAEEXAW_K4R3MRU\n",
      "Accuracy: 94.0%\n"
     ]
    }
   ],
   "source": [
    "print('\\nMy LDA')\n",
    "predict_character(pred2, 100, TRUE_LABELS_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2 == pred1[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
